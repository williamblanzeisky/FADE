# FADE: Fairness-Aware Deep Ensembles for Quantifying Uncertainty

This repository contains the code and pre-processed datasets used in the experiments. The research in this paper is motivated by a commitment to ethical principles of fairness, accountability, transparency, and ethics in AI. We recognize the potential impact of machine learning models on society and the importance of addressing bias and discrimination in these models. This research aims to ensure the fairness of deep ensemble models by explicitly considering fairness criteria in the model training process. We believe that this approach can help to reduce bias and improve the transparency and reliability of the deep ensembles model, while also ensuring that they are fair and equitable for all individuals.We evaluate our proposed method on publicly accessible datasets that contain no user-identifiable information. Finally, as part of our commitment to open research, and all of the datasets and code used in the paper are accessible on GitHub.



## Abstract
This paper brings together two important aspects of trustworthy ML that go beyond mere accuracy, that is calibration and fairness. A model is well calibrated if predictions that have a confidence of 0.6 are true 60\% of the time. For fairness, we require calibration-by-groups and no underestimation, i.e. that predictions are consistent with respect to the actual distribution, across sensitive feature values. The deep ensembles method provides uncertainty estimates, as a form of transparency, by retraining  the same neural network architecture with different weight initializations to produce a distribution of predictions. This method provides a compelling approach to approximating Bayesian predictive distributions, resulting in well-calibrated predictions. Through a series of experiments, we demonstrate that deep ensembles models are often well-calibrated in terms of posterior probabilities but less so when a sensitive attribute is involved, resulting in biased predictions. This should not come as a surprise because the distributions of weights in the deep ensembles are usually optimized to maximize generalization accuracy without explicit consideration of fairness. To address this issue, we propose Fairness-Aware Deep Ensembles (FADE), a multi-objective optimization strategy to optimize the deep ensembles model on accuracy and fairness. We empirically evaluate our framework on two synthetic and twelve real-world datasets. We find that FADE can obtain fairer models while still attaining adequate overall generalization accuracy and calibrated probability estimates.
